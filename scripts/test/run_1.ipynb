{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guochuanzhe/anaconda/envs/benchmark/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 调用环境\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader \n",
    "from compute_pretrained_embeddings import get_embeddings,get_nl_embeddings\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "class JsonlDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 这里假设 JSONL 文件中每行是一个包含文本的 JSON 对象\n",
    "        text = self.data[idx]['content']        # 要根据实际情况把数据都结合起来\n",
    "        return text\n",
    "\n",
    "# 定义DataLoader\n",
    "path_data = \"/home/guochuanzhe/Megatron-LM-gjn/data/starcoder/chatml_data/starcoder-shell-dev.jsonl\"\n",
    "dataset=JsonlDataset(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTModel(\n",
       "  (decoder): OPTDecoder(\n",
       "    (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x OPTDecoderLayer(\n",
       "        (self_attn): OPTAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载OPT-125模型及其分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "model = AutoModel.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# 开启模型的评估模式\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4383\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  2, 134],\n",
      "        [  2, 134],\n",
      "        [  2, 176]], device='cuda:0'), 'attention_mask': tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]], device='cuda:0')}\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "# 准备文本数据\n",
    "text=['1','1','2']\n",
    "# text.append(dataset[0])\n",
    "# text.append(dataset[1])\n",
    "# if(len(text)) >= 2048:\n",
    "#     continue\n",
    "# 对文本进行编码\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "# # 确保你的模型在CPU或GPU上运行\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "print(inputs)\n",
    "# 获取隐藏层状态\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "# 获取最后一个token的隐藏状态\n",
    "last_token_state = last_hidden_state[:, -1, :]\n",
    "print(last_token_state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206\n",
      "2206\n"
     ]
    }
   ],
   "source": [
    "embeddings=[]\n",
    "print(len(dataset))\n",
    "for i in range(len(dataset)):\n",
    "    # 准备文本数据\n",
    "    text = dataset[i]\n",
    "    # if(len(text)) >= 2048:\n",
    "    #     continue\n",
    "    # 对文本进行编码\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\",padding=True,truncation=True, max_length=2048)\n",
    "    # print(inputs)\n",
    "    # # 确保你的模型在CPU或GPU上运行\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 获取隐藏层状态\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "        last_hidden_state = hidden_states[-1]\n",
    "\n",
    "    # 获取最后一个token的隐藏状态\n",
    "    last_token_state = last_hidden_state[:, -1, :]\n",
    "    embeddings.append(last_token_state)\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2206 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2206 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memmap:\n",
      "[-2.54975986e-02 -7.21029937e-02  1.66285578e-02  8.26566145e-02\n",
      " -5.11781871e-02  1.25267431e-02  3.49115604e-03 -3.15127778e-04\n",
      "  5.13186380e-02  1.80433935e-03 -1.13264211e-02  1.98029988e-02\n",
      " -7.19255861e-03 -2.59566959e-02  6.11639321e-02 -3.20849307e-02\n",
      "  1.54433146e-04 -9.00619701e-02 -3.17193195e-02 -2.13452894e-02\n",
      "  1.23969754e-02 -7.36208027e-03  2.46950127e-02 -2.74652755e-03\n",
      " -7.75805339e-02  4.47592773e-02 -1.97108909e-02 -3.05027538e-03\n",
      " -4.95513082e-02 -7.45921955e-02 -8.76889005e-03 -3.20358239e-02\n",
      " -2.04661377e-02 -1.65516473e-02 -1.01848440e-02  2.02654824e-02\n",
      " -6.47709817e-02 -5.34884334e-02 -2.21250504e-02 -7.78072421e-03\n",
      "  4.10935581e-02 -7.42635690e-03 -1.40313804e-02 -2.53158156e-02\n",
      "  3.33801913e-03 -1.22634396e-02  8.68368987e-03 -5.50216846e-02\n",
      "  6.96195513e-02 -1.62526052e-02  2.96577718e-02  1.03403283e-02\n",
      " -1.44330561e-02 -1.09123765e-02 -4.12502699e-02 -2.13855077e-02\n",
      "  1.64831914e-02  1.54955555e-02 -4.64866348e-02 -3.40397027e-03\n",
      " -2.63818894e-02 -4.85018566e-02 -2.68994719e-02 -1.02881268e-02\n",
      "  1.98936090e-03  2.30391063e-02  1.73771440e-03  3.36905494e-02\n",
      "  2.34165322e-02  7.24646822e-02 -4.54106666e-02  4.52212319e-02\n",
      "  1.31493090e-02 -1.49347363e-02  1.11894747e-02  2.35223342e-02\n",
      "  1.45548629e-02 -2.58733295e-02  6.74606708e-04  6.40570652e-03\n",
      " -1.98523048e-02  3.71064879e-02 -1.51395192e-03 -3.72738838e-02\n",
      " -9.12446603e-02 -1.50873251e-02 -1.68490205e-02 -8.77824612e-03\n",
      "  1.67268906e-02 -1.54697821e-02  3.11595052e-02  2.29711216e-02\n",
      "  3.88924144e-02 -3.34014371e-02 -3.53863873e-02 -4.80744318e-04\n",
      " -3.35844010e-02 -2.33051796e-02  2.06514131e-02  3.48474234e-02\n",
      " -7.13301497e-03 -4.78651822e-02  4.00557220e-02 -5.43896742e-02\n",
      " -6.08317368e-02  2.94993725e-03 -4.59960960e-02 -4.92785266e-03\n",
      " -1.14460671e-02  3.47891934e-02  1.51239894e-02  7.72885978e-02\n",
      "  2.66997088e-02 -2.00529359e-02 -9.62235208e-04 -3.69119495e-02\n",
      "  1.61237810e-02  2.13024858e-02 -1.11605041e-02 -2.65797898e-02\n",
      " -3.34262997e-02  3.60839814e-02 -1.97489001e-02  7.83142075e-03\n",
      "  7.80832171e-02 -1.87435057e-02 -3.78453135e-02  4.65186825e-03\n",
      "  3.49089764e-02  6.80550514e-03 -2.01134570e-03  8.25033244e-03\n",
      "  4.36751358e-02  3.50709520e-02 -2.64137182e-02  4.20780107e-02\n",
      "  1.28073813e-02 -1.22899776e-02 -7.86111951e-02  1.49779553e-02\n",
      "  1.11956743e-03 -5.18559385e-03  3.99437454e-03  8.31203349e-03\n",
      "  3.55457217e-02 -9.62409154e-02  1.75897256e-02  4.52738665e-02\n",
      "  2.09438847e-03  5.10821864e-02  5.59775950e-03 -5.62823080e-02\n",
      "  6.55278638e-02  8.68126303e-02  3.78274284e-02 -1.34021919e-02\n",
      "  2.19787285e-03 -4.25231084e-02 -2.25286223e-02  6.24847934e-02\n",
      " -1.69967161e-03 -1.42509788e-02 -1.91810858e-02 -5.80872002e-04\n",
      "  3.68872173e-02 -4.78166575e-03  1.56591721e-02  2.85781194e-02\n",
      " -5.39007410e-02 -1.26784779e-02  4.63504903e-02  5.59003651e-02\n",
      " -2.75871474e-02 -5.77179752e-02 -1.85999759e-02  1.57236066e-02\n",
      " -2.58717518e-02  1.73911229e-02 -1.02619603e-02  2.40466669e-02\n",
      "  9.72854998e-03 -2.71496573e-03 -6.83032535e-03 -4.53164726e-02\n",
      "  2.72079073e-02 -3.27487513e-02  3.86578515e-02 -3.30586135e-02\n",
      " -1.54807679e-02 -1.71451475e-02 -3.00731771e-02 -2.76208352e-02\n",
      " -2.82263905e-02 -2.15076283e-02 -4.25024657e-03  4.08709608e-02\n",
      " -1.08617800e-03 -3.43366005e-02  1.90753713e-02  5.03283041e-03\n",
      " -2.68022381e-02  2.72404077e-03 -2.51632575e-02  3.62210199e-02\n",
      "  6.93738237e-02 -5.64498417e-02  4.80651995e-03  4.81518321e-02\n",
      " -4.01115008e-02 -5.96052743e-02 -4.94271182e-02 -3.18601541e-02\n",
      "  3.49291116e-02 -2.10088324e-02 -7.08382502e-02 -1.01954155e-02\n",
      " -1.07053565e-02  5.36733260e-03  1.14011271e-02  4.56455722e-02\n",
      "  3.65182478e-03  5.67242727e-02  3.15328576e-02  3.38661927e-03\n",
      "  1.19106239e-02 -2.04308406e-02 -5.34721054e-02  3.86921951e-04\n",
      "  5.09581808e-03  1.20281922e-02 -2.35489593e-03  2.46202555e-02\n",
      " -2.11800113e-02  6.18193895e-02 -1.57242417e-02 -2.02833507e-02\n",
      " -3.26477773e-02  5.18951863e-02  1.19165108e-02 -1.65134352e-02\n",
      "  4.69319262e-02  2.75469013e-02  4.67359461e-02 -6.73957318e-02\n",
      "  1.01144642e-01  2.04806216e-03  1.98359564e-02  4.84867878e-02\n",
      "  2.24147923e-02  9.23614670e-03 -2.14066505e-02 -2.34322548e-02\n",
      "  6.35471006e-05 -7.73551222e-03  2.80054100e-02  9.32259951e-03\n",
      "  2.66951621e-02 -2.89896037e-02 -9.24048107e-03 -3.16611901e-02\n",
      " -2.52318499e-03  5.29237539e-02  7.12612597e-03 -8.59242603e-02\n",
      " -3.26636396e-02 -2.08635423e-02  2.24259570e-02  1.44764688e-02\n",
      "  1.20272087e-02 -3.60153578e-02  2.57667378e-02  2.08482146e-02\n",
      "  1.67122446e-02 -9.62567050e-03  1.45128462e-02 -4.55093794e-02\n",
      " -6.00139201e-02 -2.07978133e-02 -6.42784266e-03  1.66471340e-02\n",
      " -3.81384930e-03 -1.97255332e-02 -4.39036936e-02  5.96251711e-02\n",
      " -8.27798899e-03 -3.67875211e-02 -8.40067107e-04 -1.52697181e-02\n",
      "  3.30419838e-02 -1.37595722e-04 -2.62665618e-02 -5.22441696e-04\n",
      " -3.64485313e-03  1.36458343e-02  2.07380094e-02  1.89915132e-02\n",
      " -2.61053685e-02 -4.03728820e-02  1.08696651e-02  4.44801189e-02\n",
      " -3.18643898e-02 -2.01064739e-02 -8.36978480e-03 -1.65161807e-02\n",
      " -4.81422758e-03  1.53113930e-02  8.64104405e-02  6.36568740e-02\n",
      " -1.97865209e-03 -5.02582733e-03  3.79794836e-02 -4.46890946e-03\n",
      " -1.75046399e-02 -5.57310181e-03  6.93222359e-02  2.63250731e-02\n",
      " -8.33309665e-02 -7.67179355e-02 -1.05452184e-02  5.73305041e-02\n",
      " -3.33397952e-03 -1.41727123e-02 -6.48144931e-02 -4.30589579e-02\n",
      "  4.84549738e-02 -3.37183215e-02 -2.54248809e-02  8.95272102e-03\n",
      "  3.00807133e-02 -2.31247600e-02 -3.64496782e-02 -3.06902137e-02\n",
      "  6.48363605e-02  9.47463587e-02 -8.04293305e-02  4.30855947e-03\n",
      " -1.29160155e-02 -2.10506935e-02  2.19073165e-02 -5.48277609e-02\n",
      " -4.74629365e-02 -1.66713856e-02 -8.36888887e-03 -1.81144197e-02\n",
      "  1.00020953e-02  2.16293111e-02  3.80870700e-03  3.87299433e-02\n",
      "  2.11798549e-02 -6.10052794e-02 -3.71928734e-04 -9.13579948e-03\n",
      "  5.35514913e-02  5.42484410e-02 -1.40977139e-02 -1.59032550e-02\n",
      " -1.48983058e-02 -5.53636737e-02 -1.44132487e-02 -3.20586488e-02\n",
      "  1.93020962e-02 -2.55722622e-03  1.76424701e-02 -3.97296511e-02\n",
      " -4.14820947e-02  1.61248650e-02 -3.49375047e-02  8.25456157e-03\n",
      "  2.03538872e-02 -6.41672909e-02 -6.40839804e-03  4.25397828e-02\n",
      " -2.47643348e-02  6.65725395e-03 -1.01273926e-02  2.53627226e-02\n",
      " -4.18887883e-02  6.83623599e-03 -1.14280395e-02 -3.39322560e-03\n",
      " -3.75642031e-02 -6.87369891e-03 -1.14681653e-03 -2.14644596e-02\n",
      " -5.64187532e-03  8.33012089e-02  3.71728130e-02 -1.27849402e-03\n",
      " -3.46049443e-02 -2.38233134e-02 -3.19419727e-02  1.45875029e-02\n",
      " -2.62243338e-02 -5.51882386e-03  5.08377748e-03  1.23532703e-02\n",
      " -1.24364933e-02  1.35778291e-02 -1.06995795e-02  3.18714194e-02\n",
      "  5.50318211e-02 -2.64623798e-02  6.89126253e-02  1.96623430e-02\n",
      "  7.51810372e-02 -2.42167804e-02  1.57438812e-03 -5.24070822e-02\n",
      " -1.68074127e-02 -3.75646092e-02 -2.84883119e-02 -3.97474207e-02\n",
      "  2.69380468e-03 -3.78722623e-02  2.05160007e-02 -9.56707634e-03\n",
      "  5.23815230e-02 -2.91295145e-02  1.59978010e-02 -2.55598444e-02\n",
      " -4.92075086e-03 -9.10377502e-03 -4.64498065e-03  2.23038439e-02\n",
      "  2.08913963e-02  6.29923947e-04  5.04312888e-02 -2.98260786e-02\n",
      "  2.53636111e-02 -1.76845782e-03  3.24098952e-02  2.09468286e-02\n",
      " -4.02769223e-02 -1.26767689e-02  8.29561893e-03 -4.79702801e-02\n",
      " -4.34645750e-02  1.61813870e-02 -1.38912769e-03 -2.16031503e-02\n",
      " -1.36068072e-02  3.40987816e-02 -3.51329371e-02  5.56028597e-02\n",
      "  4.01925556e-02  4.11887765e-02 -3.31791975e-02 -1.86753683e-02\n",
      "  9.79126524e-03 -3.43787894e-02 -6.94010872e-03 -2.87931645e-03\n",
      "  6.62110420e-03 -1.58543903e-02 -3.95148247e-02  4.29179110e-02\n",
      " -9.57685523e-03 -3.44523937e-02 -3.96350166e-03 -9.76615120e-03\n",
      " -3.12690698e-02  5.60610695e-03  2.77055777e-03 -1.97339375e-02\n",
      "  1.36162192e-02 -4.36018966e-03  4.69007082e-02  3.27676497e-02\n",
      "  1.86646394e-02  1.44158527e-02 -1.54791605e-02 -2.06835791e-02\n",
      " -2.62715071e-02 -2.90340884e-03  2.15983507e-03  1.66246644e-03\n",
      "  4.19893228e-02 -3.21001485e-02  9.26236995e-03  3.02563682e-02\n",
      " -1.37495389e-02  2.69019324e-02  6.42440021e-02 -2.31998004e-02\n",
      " -9.92112095e-04 -4.97553945e-02 -7.40434928e-03  5.75023666e-02\n",
      " -2.17343755e-02 -3.49724013e-03 -2.07711570e-02  4.64496724e-02\n",
      " -3.69969755e-02  1.43208564e-03 -1.69020630e-02  1.25206891e-03\n",
      "  2.62891073e-02  1.22813210e-02 -1.97905600e-02 -2.88350601e-02\n",
      " -1.28274381e-01 -3.55460346e-02  2.10670810e-02 -1.49804326e-02\n",
      " -2.47320272e-02  3.85971367e-02  7.35478178e-02  5.25662815e-03\n",
      "  1.66915990e-02  5.00776758e-03  3.85422888e-03  3.62749994e-02\n",
      "  4.49322276e-02  6.94583077e-03 -4.96555446e-03  1.94755453e-03\n",
      "  8.62546731e-03 -1.98661640e-01  4.11052816e-03  4.66176532e-02\n",
      " -4.93139029e-02 -5.50498366e-02  8.73603951e-03  9.88940969e-02\n",
      " -3.45209576e-02  4.48959600e-03  1.60285644e-02  1.45868910e-02\n",
      " -9.92063619e-03  1.09752500e-02 -4.51464504e-02 -5.26120886e-04\n",
      "  2.39983555e-02  2.54353136e-02  1.68480091e-02 -3.66173945e-02\n",
      "  6.67636003e-03 -2.85586230e-02  2.08201390e-02 -1.17085204e-02\n",
      "  1.57203134e-02  1.53942197e-03  5.08969650e-02  2.23624706e-02\n",
      " -3.44595537e-02  6.50563743e-04 -3.03474925e-02 -6.38732836e-02\n",
      " -3.47456560e-02  1.64704118e-02  2.37700213e-02 -1.66962706e-02\n",
      " -2.50286125e-02 -6.62303763e-03  2.03142711e-03  4.94523440e-03\n",
      " -2.63947416e-02  4.16294392e-03 -1.83011554e-02 -6.46680221e-02\n",
      " -3.57696265e-02 -2.10701283e-02  5.32336421e-02 -4.34326306e-02\n",
      " -2.45196834e-01 -1.00526661e-02  1.54748298e-02  8.09267047e-04\n",
      "  1.19863618e-02  5.21445367e-03 -2.69460157e-02  8.42705816e-02\n",
      " -3.40242591e-03 -4.28205356e-04  1.85395796e-02  1.43986251e-02\n",
      "  4.21650968e-02 -3.19710188e-02  4.23072055e-02  2.31044423e-02\n",
      " -2.85081640e-02  1.69336088e-02  3.77848395e-03 -7.06261620e-02\n",
      "  3.65956314e-02 -2.68916506e-02 -8.06377530e-02 -1.21968845e-02\n",
      " -6.12063222e-02 -9.80291441e-02  2.28331122e-03  8.31677914e-02\n",
      " -2.05110181e-02 -2.25285087e-02 -5.30326180e-02  7.28879822e-03\n",
      " -1.87256897e-03  4.90196515e-03  9.03289858e-03 -6.70718262e-03\n",
      " -2.59048399e-02  2.49523181e-03  1.78037174e-02  4.00424190e-02\n",
      "  6.16389960e-02  1.40787393e-03 -5.55475540e-02  3.41438055e-02\n",
      " -4.09823880e-02  3.15842330e-02 -3.86931226e-02  7.55865965e-03\n",
      "  3.37583832e-02 -2.13281717e-02 -6.14484698e-02 -1.03813726e-02\n",
      "  2.22653034e-03 -1.84392161e-03  1.43915890e-02 -5.17978854e-02\n",
      "  6.21187687e-03 -3.49521935e-02 -2.97525985e-04  3.32759283e-02\n",
      " -5.29422844e-03 -4.68707643e-02 -4.90731448e-02 -6.15762696e-02\n",
      "  1.48783699e-02  4.79604453e-02  8.74558464e-03  6.82766549e-03\n",
      "  6.02988014e-03 -1.17838262e-02 -1.72608567e-03  2.02152077e-02\n",
      "  4.26960457e-03  1.11505818e-02  6.63927794e-02 -6.86501190e-02\n",
      " -2.16233507e-02 -3.22983488e-02 -2.68301759e-02  3.99537943e-03\n",
      " -3.29545923e-02 -1.86662357e-02 -4.99080196e-02 -2.65816357e-02\n",
      "  2.74595506e-02 -3.10487337e-02  2.06381660e-02 -2.34715529e-02\n",
      " -1.24551924e-02 -7.88755622e-03  8.70387405e-02 -3.08862850e-02\n",
      "  5.12267612e-02 -5.71242720e-02  3.97081813e-03  1.60075426e-02\n",
      " -3.28485556e-02 -7.70831481e-03 -7.45667666e-02  1.63234994e-02\n",
      " -6.35452867e-02 -1.42660346e-02  9.08950493e-02  2.31940728e-02\n",
      " -1.29386298e-02  1.69365089e-02  8.50012619e-03 -3.14233243e-03\n",
      "  3.80820455e-03 -2.77516432e-02  2.06385087e-02  3.66205610e-02\n",
      " -8.49680696e-03  8.87186150e-04 -4.92907614e-02  1.70556072e-04\n",
      "  3.87013853e-02  2.22744569e-02  9.97721590e-03 -2.56197099e-02\n",
      " -3.13972309e-02 -3.80163081e-02 -5.34107201e-02 -8.64288770e-03\n",
      "  1.19282603e-02 -2.74194521e-03  3.88217643e-02 -1.45050045e-02\n",
      " -4.89458367e-02  3.98797169e-02 -8.28642249e-02  5.16081490e-02\n",
      "  1.88665986e-02  2.88757011e-02  1.01565039e-02 -9.76209808e-03\n",
      "  3.21058072e-02  4.89538908e-02 -4.84457724e-02 -7.15681389e-02\n",
      "  4.42819186e-02  4.73587133e-04 -2.27079261e-02  4.07384671e-02\n",
      "  7.12805092e-02  4.72349077e-02  2.05646772e-02  9.49429814e-03\n",
      "  4.29925844e-02  2.65794620e-03 -1.19536910e-02  3.14491726e-02\n",
      " -2.25948710e-02 -5.12623647e-03  5.86604467e-03  3.42871957e-02\n",
      "  2.12502144e-02 -4.26722504e-02  4.33501489e-02 -2.70766821e-02\n",
      " -2.07820032e-02 -6.63390905e-02  3.49231288e-02  3.73386592e-02\n",
      " -7.49271782e-03  3.82932238e-02 -4.07279395e-02 -2.05204654e-02\n",
      " -1.69183302e-03 -2.33922377e-02 -2.81302743e-02 -4.48512007e-03\n",
      "  3.51080075e-02  1.77718047e-02  1.48181980e-02  4.02678852e-05\n",
      " -7.85740931e-03  1.32159218e-02  1.31446561e-02 -6.32554665e-02\n",
      " -2.76485458e-02  3.51350494e-02 -6.14872389e-03 -5.98097965e-02\n",
      " -7.03139156e-02 -1.38086956e-02  8.64661951e-03  3.10557578e-02\n",
      " -5.48839197e-03  1.68115031e-02 -8.16835389e-02 -2.85547189e-02\n",
      "  1.03312535e-02 -4.44629490e-02  3.42742652e-02 -2.67086085e-02\n",
      " -9.46167484e-03  3.74841280e-02  3.98726203e-02 -6.35580486e-03\n",
      "  2.29131375e-02 -6.40957505e-02 -8.09101108e-03  4.58439887e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import normalize\n",
    "from tqdm.auto import tqdm \n",
    "# 定义collate_fn\n",
    "def collate_fn(examples):\n",
    "    inputs = tokenizer(examples, return_tensors=\"pt\",padding=True,truncation=True, max_length=2048)\n",
    "    return inputs\n",
    "\n",
    "# 加载Dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "path_str_type = int\n",
    "emb_memory_loc = \"/home/guochuanzhe/data-process/SemDeDup/memory/emb_memory_loc.dat\"\n",
    "paths_memory_loc = \"/home/guochuanzhe/data-process/SemDeDup/memory/paths_memory.dat\"  \n",
    "dataset_size = len(dataset)                                                           # starcoder\n",
    "emb_size = 768\n",
    "# emd_memmap = np.memmap(emb_memory_loc, dtype='float32', mode='w+', shape=(dataset_size, emb_size))\n",
    "# paths_memmap = np.memmap(paths_memory_loc, dtype=path_str_type, mode='w+', shape=(dataset_size,))\n",
    "emd_memmap = np.memmap(emb_memory_loc, dtype='float32', mode='r', shape=(dataset_size, emb_size))\n",
    "paths_memmap = np.memmap(paths_memory_loc, dtype=path_str_type, mode='r', shape=(dataset_size,))\n",
    "def get_nl_embeddings_test(model, dataloader, emd_memmap, paths_memmap):\n",
    "    \"\"\"\n",
    "    function to compute and store representations for the data from pretrained model. It is preferable to parallelize this function on mulitiple devices (GPUs). Each device will process part of the data.\n",
    "    model: pretrained model\n",
    "    dataloader: should return   1) data_batch: batch of data examples\n",
    "                                2) paths_batch: path to location where the example is stored (unique identifier). For example, this could be \"n04235860_14959.JPEG\" for imagenet.\n",
    "                                3) batch_indices: global index for each example (between 0 and of size <dataset_size>-1).\n",
    "    emd_memmap: numpy memmap to store embeddings of size <dataset_size>.\n",
    "    paths_memmap: numpy memmap to store paths of size <dataset_size>.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    count =0 \n",
    "    with torch.no_grad():\n",
    "        for data_batch in tqdm(dataloader):\n",
    "            if count >= 0:\n",
    "                break\n",
    "            # data_batch = data_batch.to(device)\n",
    "            # print(data_batch)\n",
    "            # 获取隐藏层状态\n",
    "            input_ids = data_batch['input_ids'].to(device)\n",
    "            attention_mask = data_batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            last_layer_states = hidden_states[-1]\n",
    "            last_token_state = last_layer_states[:, -1, :]\n",
    "            # 您可以在此处对 last_layer_states 进行处理，例如归一化\n",
    "            normalized_last_layer_states = normalize(last_token_state, dim=1)\n",
    "            \n",
    "            # 存储编码和路径\n",
    "\n",
    "            emd_memmap[count] = normalized_last_layer_states.cpu().numpy()\n",
    "            count +=1\n",
    "            # paths_memmap[batch_indices] = paths_batch\n",
    "\n",
    "    # 读取memmap存储的数据\n",
    "    print(\"memmap:\")\n",
    "    # emd_memmap = np.memmap(emb_memory_loc, dtype='float32', mode='r', shape=(dataset_size, emb_size))\n",
    "    # paths_memmap = np.memmap(paths_memory_loc, dtype=path_str_type, mode='r', shape=(dataset_size,))\n",
    "    print(emd_memmap[0])           \n",
    "\n",
    "# dataloader[0]\n",
    "get_nl_embeddings_test(model, dataloader, emd_memmap, paths_memmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 10431, 47813,  ...,     1,     1,     1],\n",
       "        [    2, 41552,  4147,  ..., 47992, 24303, 50140]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义collate_fn\n",
    "def collate_fn(examples):\n",
    "    inputs = tokenizer(examples, return_tensors=\"pt\",padding=True,truncation=True, max_length=2048)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
